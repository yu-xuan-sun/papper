#!/usr/bin/env python3
"""
æå–çœŸæ­£çš„é—¨æ§å€¼ï¼ˆåœ¨åŒé‡sigmoidé—®é¢˜ä¹‹å‰çš„å€¼ï¼‰
å¯¹äºå·²è®­ç»ƒçš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
1. è·å–gateç½‘ç»œçš„è¾“å‡ºï¼ˆå·²ç»ç»è¿‡ç¬¬ä¸€æ¬¡sigmoidï¼ŒèŒƒå›´0-1ï¼‰
2. è¿™ä¸ªå€¼æ‰æ˜¯çœŸæ­£åæ˜ æ¨¡æ€æƒé‡çš„æ•°å€¼
"""

import os
import sys
import json
import torch
import numpy as np
import pandas as pd
import rasterio
from tqdm import tqdm
from pathlib import Path

sys.path.insert(0, '/sunyuxuan/satbird')

# ç¯å¢ƒå˜é‡åç§°
BIOCLIM_NAMES = [
    'BIO1: Annual Mean Temp', 'BIO2: Mean Diurnal Range', 'BIO3: Isothermality',
    'BIO4: Temp Seasonality', 'BIO5: Max Temp Warmest', 'BIO6: Min Temp Coldest',
    'BIO7: Temp Annual Range', 'BIO8: Mean Temp Wettest', 'BIO9: Mean Temp Driest',
    'BIO10: Mean Temp Warmest Q', 'BIO11: Mean Temp Coldest Q', 'BIO12: Annual Precip',
    'BIO13: Precip Wettest Month', 'BIO14: Precip Driest Month', 'BIO15: Precip Seasonality',
    'BIO16: Precip Wettest Q', 'BIO17: Precip Driest Q', 'BIO18: Precip Warmest Q',
    'BIO19: Precip Coldest Q'
]

PED_NAMES = [
    'PED1: Aspect', 'PED2: Organic Carbon', 'PED3: Slope', 'PED4: TPI',
    'PED5: Clay', 'PED6: Bulk Density', 'PED7: Cation Exchange', 'PED8: Elevation'
]


def load_sample(data_dir, hotspot_id):
    """åŠ è½½å•ä¸ªæ ·æœ¬"""
    env_path = os.path.join(data_dir, 'environmental', f'{hotspot_id}.npy')
    img_path = os.path.join(data_dir, 'images', f'{hotspot_id}.tif')
    
    if not os.path.exists(env_path) or not os.path.exists(img_path):
        return None
    
    env_data = np.load(env_path).astype(np.float32)
    
    with rasterio.open(img_path) as src:
        img = src.read().astype(np.float32)
    
    img = np.clip(img / 10000.0, 0, 1)
    
    if img.shape[1] != 224 or img.shape[2] != 224:
        from torchvision.transforms.functional import resize
        img = resize(torch.from_numpy(img), [224, 224]).numpy()
    
    return {'image': torch.from_numpy(img), 'env_data': torch.from_numpy(env_data)}


def infer_params(state_dict):
    """ä»state_dictæ¨æ–­æ¨¡å‹å‚æ•°"""
    params = {}
    
    for k in state_dict.keys():
        if 'attn_adapter.down_project.weight' in k:
            params['bottleneck_dim'] = state_dict[k].shape[0]
            break
    
    for k in state_dict.keys():
        if 'prompts.0.prompts' in k:
            params['prompt_len'] = state_dict[k].shape[1]
            break
    
    for k in state_dict.keys():
        if 'fusion.env_encoder.0.weight' in k:
            params['env_hidden_dim'] = state_dict[k].shape[0]
            env_input = state_dict[k].shape[1]
            break
    
    # è®¡ç®—env_num_layers
    encoder_layers = [k for k in state_dict.keys() if 'fusion.env_encoder' in k and 'weight' in k and 'norm' not in k.lower()]
    params['env_num_layers'] = len([k for k in encoder_layers if '.weight' in k]) // 2
    
    # æ£€æŸ¥channel_adapter
    params['use_channel_adapter'] = any('channel_adapter' in k for k in state_dict.keys())
    
    return params


def extract_true_gate(model, image, env_data, device):
    """
    æå–çœŸæ­£çš„é—¨æ§å€¼ï¼ˆgateç½‘ç»œçš„åŸå§‹è¾“å‡ºï¼‰
    åœ¨æ—§ä»£ç ä¸­ï¼Œgateç½‘ç»œæœ€åæœ‰sigmoidï¼Œè¾“å‡ºåœ¨0-1ä¹‹é—´
    è¿™ä¸ªå€¼æ‰æ˜¯çœŸæ­£æœ‰æ„ä¹‰çš„æ¨¡æ€æƒé‡æŒ‡ç¤ºå™¨
    """
    model.eval()
    
    if len(image.shape) == 3:
        image = image.unsqueeze(0)
    if len(env_data.shape) == 3:
        env_data = env_data.unsqueeze(0)
    
    image = image.to(device)
    env_data = env_data.to(device)
    env_data.requires_grad_(True)
    
    with torch.no_grad():
        # é€šé“é€‚é…
        if hasattr(model, 'channel_adapter') and model.channel_adapter is not None:
            x = model.channel_adapter(image)
        else:
            x = image
        
        # è·å–è§†è§‰ç‰¹å¾
        visual_features = model.encoder(x)
        
        # è·å–ç¯å¢ƒç‰¹å¾
        env_processed = model.env_encoder(env_data)
        
        # è·å–èåˆæ¨¡å—
        fusion = model.cross_modal_fusion
        
        # æ‰‹åŠ¨æ‰§è¡Œèåˆï¼Œè·å–gateçš„åŸå§‹è¾“å‡º
        env_features = fusion.env_input_norm(env_processed)
        img_features = fusion.img_input_norm(visual_features)
        
        env_encoded = fusion.env_encoder(env_features)
        
        # è·å–gateè¾“å‡ºï¼ˆè¿™æ˜¯ç¬¬ä¸€æ¬¡sigmoidåçš„å€¼ï¼ŒèŒƒå›´0-1ï¼‰
        concat_feat = torch.cat([img_features, env_encoded], dim=1)
        gate_raw = fusion.gate(concat_feat)  # è¿™åŒ…å«äº†ç¬¬ä¸€ä¸ªsigmoid
        
        # å½“å‰forwardä¸­çš„alphaè®¡ç®—ï¼ˆåŒ…å«ç¬¬äºŒæ¬¡sigmoidï¼‰
        temp = fusion.temperature.abs().clamp(min=0.01)
        alpha_final = torch.sigmoid(gate_raw / temp)
        
        return {
            'gate_raw': gate_raw.cpu().item(),  # ç¬¬ä¸€æ¬¡sigmoidåçš„å€¼ï¼ˆæœ‰æ„ä¹‰ï¼‰
            'alpha_final': alpha_final.cpu().item(),  # æœ€ç»ˆalphaï¼ˆæ¥è¿‘1ï¼‰
            'temperature': temp.cpu().item()
        }


def extract_with_gradients(model, image, env_data, device):
    """æå–ç¯å¢ƒå˜é‡æ¢¯åº¦é‡è¦æ€§"""
    model.eval()
    
    if len(image.shape) == 3:
        image = image.unsqueeze(0)
    if len(env_data.shape) == 3:
        env_data = env_data.unsqueeze(0)
    
    image = image.to(device)
    env_data = env_data.to(device).requires_grad_(True)
    
    # å‰å‘ä¼ æ’­
    output = model(image, env_data)
    
    # è®¡ç®—æ¢¯åº¦
    target = output.abs().sum()
    target.backward()
    
    env_grad = env_data.grad.detach()
    
    # ç¯å¢ƒå˜é‡é‡è¦æ€§
    env_importance = (env_grad.abs() * env_data.detach().abs()).mean(dim=(2, 3)).cpu().numpy()[0]
    
    # è·å–gateå€¼
    gate_info = extract_true_gate(model, image, env_data.detach(), device)
    
    return {
        'gate_raw': gate_info['gate_raw'],
        'alpha_final': gate_info['alpha_final'],
        'temperature': gate_info['temperature'],
        'env_importance': env_importance
    }


def analyze_domain(name, data_dir, ckpt_path, max_samples=2000, device='cuda'):
    """åˆ†æå•ä¸ªåŸŸ"""
    print(f"\n{'='*70}")
    print(f"åˆ†æåŸŸ: {name}")
    print(f"{'='*70}")
    
    from src.models.dinov2_adapter_prompt import Dinov2AdapterPrompt
    
    if not os.path.exists(ckpt_path):
        print(f"  Checkpointä¸å­˜åœ¨: {ckpt_path}")
        return None
    
    # åŠ è½½checkpoint
    ckpt = torch.load(ckpt_path, map_location='cpu')
    state_dict = ckpt.get('state_dict', ckpt)
    
    # å¤„ç†key
    new_state_dict = {}
    for k, v in state_dict.items():
        new_k = k[6:] if k.startswith('model.') else k
        new_state_dict[new_k] = v
    
    # æ¨æ–­å‚æ•°
    params = infer_params(new_state_dict)
    
    # è·å–num_classes
    for k, v in new_state_dict.items():
        if 'classifier' in k and 'weight' in k and v.dim() == 2:
            if v.shape[0] > 100:
                num_classes = v.shape[0]
    
    # è·å–env_dim
    for k, v in new_state_dict.items():
        if 'fusion.env_encoder.0.weight' in k:
            env_dim = v.shape[1]
            break
    
    print(f"  num_classes={num_classes}, env_dim={env_dim}")
    print(f"  params: {params}")
    
    # åˆ›å»ºæ¨¡å‹
    model = Dinov2AdapterPrompt(
        num_classes=num_classes,
        prompt_len=params.get('prompt_len', 40),
        bottleneck_dim=params.get('bottleneck_dim', 96),
        env_input_dim=env_dim,
        env_hidden_dim=params.get('env_hidden_dim', 2048),
        env_num_layers=params.get('env_num_layers', 3),
        use_env=True,
        use_channel_adapter=params.get('use_channel_adapter', True),
        in_channels=4
    )
    
    msg = model.load_state_dict(new_state_dict, strict=False)
    print(f"  åŠ è½½: {len(msg.missing_keys)} missing, {len(msg.unexpected_keys)} unexpected")
    
    model = model.to(device)
    model.eval()
    
    # åŠ è½½æ•°æ®
    split_file = os.path.join(data_dir, 'test_split.csv')
    df = pd.read_csv(split_file)
    total_samples = len(df)
    hotspot_ids = df['hotspot_id'].tolist()[:max_samples]
    
    print(f"  åˆ†ææ ·æœ¬: {len(hotspot_ids)}/{total_samples}")
    
    # åˆ†æ
    gate_raws = []
    alpha_finals = []
    all_importance = []
    
    for hid in tqdm(hotspot_ids, desc=f"åˆ†æ {name}"):
        sample = load_sample(data_dir, hid)
        if sample is None:
            continue
        
        try:
            info = extract_with_gradients(model, sample['image'], sample['env_data'], device)
            gate_raws.append(info['gate_raw'])
            alpha_finals.append(info['alpha_final'])
            all_importance.append(info['env_importance'])
        except Exception as e:
            continue
        
        model.zero_grad()
    
    print(f"\n  æˆåŠŸåˆ†æ: {len(all_importance)} ä¸ªæ ·æœ¬")
    
    if len(all_importance) == 0:
        return None
    
    # è®¡ç®—ç»Ÿè®¡
    gate_raws = np.array(gate_raws)
    alpha_finals = np.array(alpha_finals)
    all_importance = np.array(all_importance)
    
    mean_importance = all_importance.mean(axis=0)
    mean_importance_norm = mean_importance / (mean_importance.sum() + 1e-8)
    
    top5_idx = np.argsort(mean_importance_norm)[::-1][:5].tolist()
    
    env_names = BIOCLIM_NAMES[:env_dim] if env_dim <= 19 else BIOCLIM_NAMES + PED_NAMES[:env_dim-19]
    
    print(f"\n  Gateç»Ÿè®¡ (çœŸæ­£çš„æ¨¡æ€æƒé‡æŒ‡ç¤º):")
    print(f"    Gate Raw: {gate_raws.mean():.4f} Â± {gate_raws.std():.4f}")
    print(f"    Range: [{gate_raws.min():.4f}, {gate_raws.max():.4f}]")
    print(f"    (å€¼>0.5è¡¨ç¤ºåå‘ç¯å¢ƒç‰¹å¾ï¼Œ<0.5è¡¨ç¤ºåå‘è§†è§‰ç‰¹å¾)")
    
    print(f"\n  Alpha Final (å—åŒé‡sigmoidå½±å“ï¼Œæ¥è¿‘1):")
    print(f"    Mean: {alpha_finals.mean():.6f}")
    
    print(f"\n  Top 5 ç¯å¢ƒå˜é‡:")
    for i, idx in enumerate(top5_idx):
        print(f"    {i+1}. [{idx:2d}] {env_names[idx]}: {mean_importance_norm[idx]:.4f}")
    
    return {
        'domain': name,
        'num_samples': len(all_importance),
        'total_samples': total_samples,
        'env_dim': env_dim,
        'env_names': env_names,
        'gate_stats': {
            'mean': float(gate_raws.mean()),
            'std': float(gate_raws.std()),
            'min': float(gate_raws.min()),
            'max': float(gate_raws.max()),
            'interpretation': '>0.5 = env-dominant, <0.5 = visual-dominant'
        },
        'alpha_stats': {
            'mean': float(alpha_finals.mean()),
            'note': 'Affected by double-sigmoid bug, always near 1'
        },
        'env_importance': {
            'mean_importance': mean_importance_norm.tolist(),
            'top_5_indices': top5_idx,
            'top_5_importance': [float(mean_importance_norm[i]) for i in top5_idx],
            'top_5_names': [env_names[i] for i in top5_idx]
        }
    }


def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")
    
    # åŸŸé…ç½®
    configs = {
        'USA_Summer': {
            'data_dir': '/sunyuxuan/satbird/USA_summer',
            'checkpoint': '/sunyuxuan/satbird/runs/dinov2_v10_nir_enhanced_summer_seed42_20251201-153435/checkpoints/best-56-0.0522.ckpt'
        },
        'USA_Winter': {
            'data_dir': '/sunyuxuan/satbird/USA_winter',
            'checkpoint': '/sunyuxuan/satbird/runs/best winter/checkpoints/best-80-0.0479.ckpt'
        },
        'Kenya_Transfer': {
            'data_dir': '/sunyuxuan/satbird/kenya',
            'checkpoint': '/sunyuxuan/satbird/runs/transfer_usa_to_kenya_linear_seed42_20251202-023844/checkpoints/best-73-0.0694.ckpt'
        }
    }
    
    results = {}
    
    for name, cfg in configs.items():
        try:
            result = analyze_domain(
                name=name,
                data_dir=cfg['data_dir'],
                ckpt_path=cfg['checkpoint'],
                max_samples=2000,
                device=device
            )
            if result:
                results[name] = result
        except Exception as e:
            print(f"åŸŸ {name} åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜ç»“æœ
    out_dir = '/sunyuxuan/satbird/outputs/interpretability_v2'
    os.makedirs(out_dir, exist_ok=True)
    
    for name, result in results.items():
        with open(os.path.join(out_dir, f'{name}_analysis.json'), 'w') as f:
            json.dump(result, f, indent=2)
    
    # æ±‡æ€»
    print("\n" + "="*70)
    print("ğŸ“Š åˆ†ææ±‡æ€»")
    print("="*70)
    
    for name, result in results.items():
        print(f"\n{name}:")
        print(f"  æ ·æœ¬: {result['num_samples']}/{result['total_samples']} ({result['num_samples']/result['total_samples']*100:.1f}%)")
        print(f"  Gate Raw: {result['gate_stats']['mean']:.4f} Â± {result['gate_stats']['std']:.4f}")
        print(f"  Top 3: {', '.join(result['env_importance']['top_5_names'][:3])}")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {out_dir}")


if __name__ == '__main__':
    main()
