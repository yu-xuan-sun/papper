#!/usr/bin/env python3
"""
EVP (Environment-aware Visual Prompt Tuning) è®­ç»ƒè„šæœ¬
æ”¯æŒåœ¨åŸæœ‰DINOv2 Adapter+Promptæ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ EVPåŠŸèƒ½
"""

import os
import sys
sys.path.insert(0, '/sunyuxuan/satbird')

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Dict, Tuple
import copy

# å¯¼å…¥åŸºç¡€æ¨¡å‹ç»„ä»¶
from src.models.dinov2_adapter_prompt import Dinov2AdapterPrompt
from src.models.env_aware_prompt import EnvironmentPromptGenerator


class EVPDinov2Model(nn.Module):
    """
    å¸¦EVPçš„DINOv2æ¨¡å‹
    åœ¨åŸæœ‰Dinov2AdapterPromptåŸºç¡€ä¸Šï¼Œå°†é™æ€Promptæ›¿æ¢ä¸ºç¯å¢ƒæ„ŸçŸ¥çš„åŠ¨æ€Prompt
    """
    
    def __init__(
        self,
        base_model: Dinov2AdapterPrompt,
        env_dim: int = 27,
        use_evp: bool = True,
        evp_hidden_dim: int = 512,
        evp_dropout: float = 0.1,
        freeze_base: bool = False
    ):
        super().__init__()
        
        self.base_model = base_model
        self.use_evp = use_evp
        self.env_dim = env_dim
        
        # è·å–åŸºç¡€æ¨¡å‹å‚æ•°
        self.embed_dim = base_model.embed_dim
        self.prompt_len = base_model.prompt_len
        self.num_layers = len(base_model.dino.blocks)
        
        if use_evp:
            # åˆ›å»ºEVPç”Ÿæˆå™¨
            self.evp_generator = EnvironmentPromptGenerator(
                env_dim=env_dim,
                prompt_len=self.prompt_len,
                embed_dim=self.embed_dim,
                hidden_dim=evp_hidden_dim,
                num_layers=self.num_layers,
                use_layer_specific=True,
                use_residual=True,
                use_gating=True,
                dropout=evp_dropout
            )
            print(f"âœ¨ EVP enabled: env_dim={env_dim}, prompt_len={self.prompt_len}")
        
        if freeze_base:
            # å†»ç»“åŸºç¡€æ¨¡å‹ï¼Œåªè®­ç»ƒEVP
            for param in self.base_model.parameters():
                param.requires_grad = False
            print("ğŸ”’ Base model frozen, only training EVP")
        
        self._print_params()
    
    def _print_params(self):
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ“Š Parameters: {trainable:,} trainable / {total:,} total ({100*trainable/total:.2f}%)")
    
    def forward(
        self, 
        img: torch.Tensor, 
        env: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            img: [B, C, H, W] è¾“å…¥å›¾åƒ
            env: [B, env_dim] ç¯å¢ƒç‰¹å¾
            
        Returns:
            logits: [B, num_classes] åˆ†ç±»è¾“å‡º
        """
        batch_size = img.size(0)
        
        # Channel adapter
        if self.base_model.channel_adapter is not None:
            x = self.base_model.channel_adapter(img)
        elif self.base_model.in_channels > 3:
            x = img[:, :3, :, :]
        else:
            x = img
        
        # Patch embedding
        x = self.base_model.dino.patch_embed(x)
        
        # Add CLS token
        if self.base_model.dino.cls_token is not None:
            cls_token = self.base_model.dino.cls_token.expand(batch_size, -1, -1)
            x = torch.cat([cls_token, x], dim=1)
        
        x = self.base_model.dino.pos_drop(x)
        
        # ä½¿ç”¨EVPæˆ–é™æ€Prompt
        if self.use_evp and env is not None:
            # EVP: æ ¹æ®ç¯å¢ƒç‰¹å¾åŠ¨æ€ç”ŸæˆPrompt
            for i, block in enumerate(self.base_model.dino.blocks):
                # ç”Ÿæˆç¯å¢ƒæ„ŸçŸ¥çš„prompts
                prompts, gate = self.evp_generator(env, layer_idx=i)
                
                # æ·»åŠ promptsåˆ°åºåˆ—
                x_with_prompts = torch.cat([x, prompts], dim=1)
                
                # é€šè¿‡Transformer block
                x_with_prompts = block(x_with_prompts)
                
                # ç§»é™¤prompts
                x = x_with_prompts[:, :-self.prompt_len, :]
        else:
            # ä½¿ç”¨åŸå§‹çš„é™æ€Prompt (fallback)
            x = self.base_model.adapted_encoder(x)
        
        # Final normalization
        x = self.base_model.dino.norm(x)
        
        # å–CLS token
        visual_feat = x[:, 0]
        
        # èåˆç¯å¢ƒç‰¹å¾
        if self.base_model.use_env and env is not None:
            if self.base_model.fusion_type == "adaptive_attention":
                fused_feat = self.base_model.fusion(visual_feat, env)
            elif self.base_model.fusion_type == "cross_attention":
                env_feat = self.base_model.env_encoder(env)
                fused_feat = self.base_model.fusion(visual_feat, env_feat)
            elif self.base_model.fusion_type == "concat":
                env_feat = self.base_model.env_encoder(env)
                fused_feat = torch.cat([visual_feat, env_feat], dim=1)
            else:
                fused_feat = visual_feat
        else:
            fused_feat = visual_feat
        
        # åˆ†ç±»å¤´
        logits = self.base_model.classifier(fused_feat)
        
        return logits
    
    def get_evp_gate_values(self, env: torch.Tensor) -> List[torch.Tensor]:
        """è·å–æ‰€æœ‰å±‚çš„EVPé—¨æ§å€¼ï¼Œç”¨äºå¯è§£é‡Šæ€§åˆ†æ"""
        if not self.use_evp:
            return []
        
        _, all_gates = self.evp_generator.get_all_prompts(env)
        return all_gates


def create_evp_model(
    checkpoint_path: str = None,
    num_classes: int = 624,
    env_dim: int = 27,
    in_channels: int = 4,
    use_evp: bool = True,
    freeze_base: bool = False,
    device: str = 'cuda'
) -> EVPDinov2Model:
    """
    åˆ›å»ºEVPæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        checkpoint_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (å¯é€‰)
        num_classes: ç±»åˆ«æ•°
        env_dim: ç¯å¢ƒç‰¹å¾ç»´åº¦
        in_channels: è¾“å…¥é€šé“æ•°
        use_evp: æ˜¯å¦ä½¿ç”¨EVP
        freeze_base: æ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹
        device: è®¾å¤‡
        
    Returns:
        EVPDinov2Model
    """
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    base_model = Dinov2AdapterPrompt(
        num_classes=num_classes,
        dino_model_name='vit_base_patch14_dinov2.lvd142m',
        pretrained_path='checkpoints/dinov2_vitb14_pretrain.pth',
        prompt_len=40,
        bottleneck_dim=96,
        adapter_layers=None,  # æ‰€æœ‰å±‚
        adapter_dropout=0.1,
        env_input_dim=env_dim,
        env_hidden_dim=2048,
        env_num_layers=3,
        use_env=True,
        fusion_type='adaptive_attention',
        hidden_dims=[2048, 1024],
        dropout=0.15,
        use_channel_adapter=True,
        in_channels=in_channels,
        freeze_backbone=True,
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ (å¦‚æœæœ‰)
    if checkpoint_path and os.path.exists(checkpoint_path):
        print(f"ğŸ“¥ Loading checkpoint from {checkpoint_path}")
        state_dict = torch.load(checkpoint_path, map_location='cpu')
        if 'state_dict' in state_dict:
            # PyTorch Lightningæ ¼å¼
            state_dict = {k.replace('model.', ''): v for k, v in state_dict['state_dict'].items()}
        base_model.load_state_dict(state_dict, strict=False)
    
    # åˆ›å»ºEVPæ¨¡å‹
    evp_model = EVPDinov2Model(
        base_model=base_model,
        env_dim=env_dim,
        use_evp=use_evp,
        evp_hidden_dim=512,
        evp_dropout=0.1,
        freeze_base=freeze_base
    )
    
    return evp_model.to(device)


def test_evp_model():
    """æµ‹è¯•EVPæ¨¡å‹"""
    print("="*60)
    print("Testing EVP Model")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_evp_model(
        checkpoint_path=None,  # ä¸åŠ è½½é¢„è®­ç»ƒ
        num_classes=624,
        env_dim=27,
        in_channels=4,
        use_evp=True,
        freeze_base=False,
        device=device
    )
    
    model.eval()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    img = torch.randn(batch_size, 4, 224, 224).to(device)
    env = torch.randn(batch_size, 27).to(device)
    
    with torch.no_grad():
        output = model(img, env)
    
    print(f"\nâœ… Forward pass successful!")
    print(f"   Input image shape: {img.shape}")
    print(f"   Input env shape: {env.shape}")
    print(f"   Output shape: {output.shape}")
    
    # æµ‹è¯•EVPé—¨æ§å€¼
    gate_values = model.get_evp_gate_values(env)
    print(f"\nğŸ“Š EVP Gate values:")
    for i, gate in enumerate(gate_values[:3]):
        print(f"   Layer {i}: {gate.mean().item():.4f}")
    print(f"   ... ({len(gate_values)} layers total)")
    
    # æµ‹è¯•æ¢¯åº¦æµ
    model.train()
    output = model(img, env)
    loss = output.sum()
    loss.backward()
    
    evp_grad_norm = 0
    for name, param in model.named_parameters():
        if 'evp_generator' in name and param.grad is not None:
            evp_grad_norm += param.grad.norm().item()
    
    print(f"\nğŸ”„ Gradient flow:")
    print(f"   EVP gradient norm: {evp_grad_norm:.4f}")
    
    print("\n" + "="*60)
    print("âœ… All EVP Model tests passed!")
    print("="*60)
    
    return model


if __name__ == '__main__':
    test_evp_model()
