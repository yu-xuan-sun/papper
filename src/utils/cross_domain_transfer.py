"""
è·¨åŸŸè¿ç§»å­¦ä¹ å·¥å…·
æ”¯æŒä»Žä¸€ä¸ªæ•°æ®é›†è¿ç§»åˆ°å¦ä¸€ä¸ªæ•°æ®é›†
å¤„ç†ç‰©ç§æ•°ä¸åŒã€çŽ¯å¢ƒå˜é‡ç»´åº¦ä¸åŒçš„æƒ…å†µ
"""

import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
from collections import OrderedDict


class CrossDomainTransfer:
    """è·¨åŸŸè¿ç§»å­¦ä¹ ç®¡ç†å™¨"""
    
    # å¯è¿ç§»çš„å±‚ (ä¸Žæ•°æ®é›†æ— å…³)
    TRANSFERABLE_PATTERNS = [
        'dino',               # DINOv2 backbone
        'backbone',           # é€šç”¨backbone
        'channel_adapter',    # é€šé“é€‚é…å™¨
        'adapted_encoder',    # é€‚é…åŽçš„ç¼–ç å™¨
        'attn_adapter',       # æ³¨æ„åŠ›adapter
        'mlp_adapter',        # MLP adapter
        'prompt',             # Promptæ¨¡å—
        'patch_embed',        # Patch embedding
        'cls_token',          # CLS token
        'pos_embed',          # Position embedding
    ]
    
    # ä¸å¯è¿ç§»çš„å±‚ (ä¸Žæ•°æ®é›†ç›¸å…³)
    NON_TRANSFERABLE_PATTERNS = [
        'classifier',         # åˆ†ç±»å¤´ (ç‰©ç§æ•°ç›¸å…³)
        'env_encoder',        # çŽ¯å¢ƒç¼–ç å™¨ (çŽ¯å¢ƒå˜é‡ç»´åº¦ç›¸å…³)
        'env_input_norm',     # çŽ¯å¢ƒè¾“å…¥å½’ä¸€åŒ–
        'fusion.env',         # èžåˆæ¨¡å—çš„çŽ¯å¢ƒéƒ¨åˆ†
    ]
    
    @staticmethod
    def load_transferable_weights(
        model: nn.Module,
        source_checkpoint: str,
        transfer_mode: str = 'freeze_backbone',
        strict: bool = False,
        verbose: bool = True
    ) -> Tuple[List[str], List[str]]:
        """
        åŠ è½½å¯è¿ç§»çš„æƒé‡
        
        Args:
            model: ç›®æ ‡æ¨¡åž‹
            source_checkpoint: æºcheckpointè·¯å¾„
            transfer_mode: è¿ç§»æ¨¡å¼
                - 'freeze_backbone': å†»ç»“backboneï¼Œåªè®­ç»ƒåˆ†ç±»å¤´
                - 'finetune_all': åŠ è½½æƒé‡åŽå…¨éƒ¨å¯è®­ç»ƒ
                - 'linear_probe': åªè®­ç»ƒæœ€åŽçš„åˆ†ç±»å±‚
            strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            (loaded_keys, skipped_keys)
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"è·¨åŸŸæƒé‡è¿ç§»")
            print(f"{'='*60}")
            print(f"æºcheckpoint: {source_checkpoint}")
            print(f"è¿ç§»æ¨¡å¼: {transfer_mode}")
        
        # åŠ è½½æºcheckpoint
        checkpoint = torch.load(source_checkpoint, map_location='cpu')
        if 'state_dict' in checkpoint:
            source_state = checkpoint['state_dict']
        else:
            source_state = checkpoint
        
        # å¤„ç†model.å‰ç¼€ - å¦‚æžœæºcheckpointä¸­æœ‰model.å‰ç¼€ï¼ŒåŽ»æŽ‰å®ƒ
        processed_source_state = {}
        for key, value in source_state.items():
            # åŽ»æŽ‰'model.'å‰ç¼€
            if key.startswith('model.'):
                new_key = key[6:]  # åŽ»æŽ‰'model.'
            else:
                new_key = key
            processed_source_state[new_key] = value
        
        source_state = processed_source_state
        
        # èŽ·å–æ¨¡åž‹å½“å‰state
        model_state = model.state_dict()
        
        loaded_keys = []
        skipped_keys = []
        shape_mismatch_keys = []
        
        for key, source_value in source_state.items():
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ (ä¸å¯è¿ç§»çš„å±‚)
            should_skip = any(
                pattern in key 
                for pattern in CrossDomainTransfer.NON_TRANSFERABLE_PATTERNS
            )
            
            if should_skip:
                skipped_keys.append((key, "excluded (domain-specific)"))
                continue
            
            # æ£€æŸ¥keyæ˜¯å¦å­˜åœ¨äºŽç›®æ ‡æ¨¡åž‹
            if key not in model_state:
                skipped_keys.append((key, "not in target model"))
                continue
            
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
            target_value = model_state[key]
            if source_value.shape != target_value.shape:
                shape_mismatch_keys.append((key, source_value.shape, target_value.shape))
                skipped_keys.append((key, f"shape mismatch {source_value.shape} vs {target_value.shape}"))
                continue
            
            # åŠ è½½æƒé‡
            model_state[key] = source_value
            loaded_keys.append(key)
        
        # åº”ç”¨æƒé‡
        model.load_state_dict(model_state, strict=False)
        
        if verbose:
            print(f"\nâœ… æˆåŠŸåŠ è½½: {len(loaded_keys)} ä¸ªå‚æ•°")
            print(f"â­ï¸  è·³è¿‡: {len(skipped_keys)} ä¸ªå‚æ•°")
            
            if shape_mismatch_keys:
                print(f"\nâš ï¸  ç»´åº¦ä¸åŒ¹é…çš„å‚æ•°:")
                for key, src_shape, tgt_shape in shape_mismatch_keys[:5]:
                    print(f"   {key}: {src_shape} â†’ {tgt_shape}")
                if len(shape_mismatch_keys) > 5:
                    print(f"   ... å…± {len(shape_mismatch_keys)} ä¸ª")
            
            # æ˜¾ç¤ºè·³è¿‡çš„domain-specificå‚æ•°
            domain_specific = [(k, r) for k, r in skipped_keys if 'domain-specific' in r]
            if domain_specific:
                print(f"\nðŸ“‹ Domain-specificå‚æ•° (é‡æ–°åˆå§‹åŒ–):")
                for k, _ in domain_specific[:5]:
                    print(f"   {k}")
                if len(domain_specific) > 5:
                    print(f"   ... å…± {len(domain_specific)} ä¸ª")
        
        # æ ¹æ®è¿ç§»æ¨¡å¼è®¾ç½®å†»ç»“ç­–ç•¥
        CrossDomainTransfer._apply_freeze_strategy(model, transfer_mode, verbose)
        
        return loaded_keys, [k[0] for k in skipped_keys]
    
    @staticmethod
    def _apply_freeze_strategy(
        model: nn.Module, 
        transfer_mode: str,
        verbose: bool = True
    ):
        """åº”ç”¨å†»ç»“ç­–ç•¥"""
        
        if transfer_mode == 'freeze_backbone':
            # å†»ç»“æ‰€æœ‰å·²è¿ç§»çš„å±‚ï¼Œåªè®­ç»ƒæ–°åˆå§‹åŒ–çš„å±‚
            frozen_count = 0
            trainable_count = 0
            
            for name, param in model.named_parameters():
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¯è¿ç§»çš„(éœ€è¦å†»ç»“çš„)å±‚
                should_freeze = any(
                    pattern in name 
                    for pattern in CrossDomainTransfer.TRANSFERABLE_PATTERNS
                )
                
                if should_freeze:
                    param.requires_grad = False
                    frozen_count += 1
                else:
                    param.requires_grad = True
                    trainable_count += 1
            
            if verbose:
                print(f"\nðŸ”’ å†»ç»“ç­–ç•¥: freeze_backbone")
                print(f"   å†»ç»“å‚æ•°ç»„: {frozen_count}")
                print(f"   å¯è®­ç»ƒå‚æ•°ç»„: {trainable_count}")
                
        elif transfer_mode == 'linear_probe':
            # åªè®­ç»ƒåˆ†ç±»å™¨
            frozen = 0
            trainable = 0
            for name, param in model.named_parameters():
                if 'classifier' in name:
                    param.requires_grad = True
                    trainable += 1
                else:
                    param.requires_grad = False
                    frozen += 1
            
            if verbose:
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                print(f"\nðŸ”’ å†»ç»“ç­–ç•¥: linear_probe")
                print(f"   åªè®­ç»ƒåˆ†ç±»å¤´ï¼Œå¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
                print(f"   å†»ç»“å‚æ•°ç»„: {frozen}, å¯è®­ç»ƒå‚æ•°ç»„: {trainable}")
                
        elif transfer_mode == 'finetune_all':
            # å…¨éƒ¨å¯è®­ç»ƒ
            for param in model.parameters():
                param.requires_grad = True
            
            if verbose:
                total = sum(p.numel() for p in model.parameters())
                print(f"\nï¿½ï¿½ å†»ç»“ç­–ç•¥: finetune_all")
                print(f"   å…¨éƒ¨å¯è®­ç»ƒï¼Œå‚æ•°é‡: {total:,}")
        
        else:
            raise ValueError(f"Unknown transfer_mode: {transfer_mode}")
    
    @staticmethod
    def get_parameter_groups(
        model: nn.Module,
        base_lr: float = 1e-4,
        backbone_lr_scale: float = 0.1,
        new_layer_lr_scale: float = 10.0
    ) -> List[Dict]:
        """
        èŽ·å–åˆ†å±‚å­¦ä¹ çŽ‡çš„å‚æ•°ç»„
        
        Args:
            model: æ¨¡åž‹
            base_lr: åŸºç¡€å­¦ä¹ çŽ‡
            backbone_lr_scale: backboneå­¦ä¹ çŽ‡ç¼©æ”¾å› å­
            new_layer_lr_scale: æ–°å±‚å­¦ä¹ çŽ‡ç¼©æ”¾å› å­
            
        Returns:
            å‚æ•°ç»„åˆ—è¡¨ï¼Œç”¨äºŽoptimizer
        """
        backbone_params = []
        new_layer_params = []
        other_params = []
        
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
                
            is_new_layer = any(
                pattern in name 
                for pattern in CrossDomainTransfer.NON_TRANSFERABLE_PATTERNS
            )
            
            is_backbone = any(
                pattern in name
                for pattern in ['dino', 'backbone', 'patch_embed', 'cls_token', 'pos_embed']
            )
            
            if is_new_layer:
                new_layer_params.append(param)
            elif is_backbone:
                backbone_params.append(param)
            else:
                other_params.append(param)
        
        param_groups = []
        
        if backbone_params:
            param_groups.append({
                'params': backbone_params,
                'lr': base_lr * backbone_lr_scale,
                'name': 'backbone'
            })
        
        if other_params:
            param_groups.append({
                'params': other_params,
                'lr': base_lr,
                'name': 'adapter_prompt'
            })
        
        if new_layer_params:
            param_groups.append({
                'params': new_layer_params,
                'lr': base_lr * new_layer_lr_scale,
                'name': 'new_layers'
            })
        
        return param_groups


def transfer_and_freeze(
    model: nn.Module,
    source_checkpoint: str,
    transfer_mode: str = 'freeze_backbone'
) -> nn.Module:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŠ è½½æƒé‡å¹¶åº”ç”¨å†»ç»“ç­–ç•¥
    
    Example:
        model = create_model(kenya_config)
        model = transfer_and_freeze(model, 'usa_model.ckpt', 'freeze_backbone')
    """
    CrossDomainTransfer.load_transferable_weights(
        model, source_checkpoint, transfer_mode
    )
    return model


# =====================================================
# æµ‹è¯•å‡½æ•°
# =====================================================
def test_transfer():
    """æµ‹è¯•è·¨åŸŸè¿ç§»åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•è·¨åŸŸè¿ç§»åŠŸèƒ½")
    print("=" * 60)
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    pass


if __name__ == "__main__":
    test_transfer()


# Additional transfer modes for HGCP+FDA model
class HGCPFDATransfer(CrossDomainTransfer):
    """Extended transfer support for HGCP+FDA model"""
    
    # HGCP+FDA specific patterns
    HGCP_PATTERNS = ['hgcp', 'hierarchical', 'geo_prompt', 'gate']
    FDA_PATTERNS = ['fda', 'freq', 'low_freq', 'high_freq', 'frequency']
    ADAPTER_PATTERNS = ['adapter', 'prompt', 'channel_adapter']
    BACKBONE_PATTERNS = ['dino', 'backbone', 'patch_embed', 'cls_token', 'pos_embed', 'blocks']
    
    @staticmethod
    def apply_transfer_strategy(
        model: nn.Module,
        transfer_mode: str,
        unfreeze_last_n_blocks: int = 0,
        verbose: bool = True
    ) -> Dict[str, int]:
        """
        Apply transfer learning strategy for HGCP+FDA model
        
        Args:
            model: The model to configure
            transfer_mode: One of ['linear_probe', 'adapter_tune', 'finetune']
            unfreeze_last_n_blocks: Number of last backbone blocks to unfreeze
            verbose: Print detailed info
            
        Returns:
            Dict with counts of frozen/trainable parameters
        """
        stats = {'frozen': 0, 'trainable': 0, 'total_params': 0}
        
        if verbose:
            print(f"\n{'='*50}")
            print(f"Transfer Strategy: {transfer_mode}")
            print(f"{'='*50}")
        
        if transfer_mode == 'linear_probe':
            # Only train classifier, freeze everything else
            for name, param in model.named_parameters():
                if 'classifier' in name or 'fc' in name:
                    param.requires_grad = True
                    stats['trainable'] += param.numel()
                else:
                    param.requires_grad = False
                    stats['frozen'] += param.numel()
                stats['total_params'] += param.numel()
                    
        elif transfer_mode == 'adapter_tune':
            # Freeze backbone, train adapters + HGCP + FDA + classifier
            for name, param in model.named_parameters():
                # Check if it's a backbone parameter (to freeze)
                is_backbone = any(
                    pattern in name.lower() 
                    for pattern in HGCPFDATransfer.BACKBONE_PATTERNS
                )
                # Check if it's an adapter/HGCP/FDA parameter (to train)
                is_adapter = any(
                    pattern in name.lower()
                    for pattern in HGCPFDATransfer.ADAPTER_PATTERNS + 
                                   HGCPFDATransfer.HGCP_PATTERNS +
                                   HGCPFDATransfer.FDA_PATTERNS
                )
                is_classifier = 'classifier' in name.lower() or 'fc' in name.lower()
                is_env_encoder = 'env' in name.lower()
                
                # Handle backbone blocks with unfreeze_last_n_blocks
                if is_backbone and 'blocks' in name.lower() and unfreeze_last_n_blocks > 0:
                    # Extract block number
                    import re
                    block_match = re.search(r'blocks\.(\d+)', name)
                    if block_match:
                        block_num = int(block_match.group(1))
                        total_blocks = 12  # DINOv2 ViT-B has 12 blocks
                        if block_num >= total_blocks - unfreeze_last_n_blocks:
                            is_backbone = False  # Unfreeze this block
                
                if is_backbone and not is_adapter:
                    param.requires_grad = False
                    stats['frozen'] += param.numel()
                else:
                    param.requires_grad = True
                    stats['trainable'] += param.numel()
                stats['total_params'] += param.numel()
                    
        elif transfer_mode == 'finetune':
            # Train all with differential learning rates
            # Partially unfreeze backbone (last n blocks)
            for name, param in model.named_parameters():
                is_backbone = any(
                    pattern in name.lower() 
                    for pattern in HGCPFDATransfer.BACKBONE_PATTERNS
                )
                
                # Handle backbone blocks with unfreeze_last_n_blocks
                should_freeze_backbone = is_backbone and 'blocks' in name.lower()
                if should_freeze_backbone and unfreeze_last_n_blocks > 0:
                    import re
                    block_match = re.search(r'blocks\.(\d+)', name)
                    if block_match:
                        block_num = int(block_match.group(1))
                        total_blocks = 12
                        if block_num >= total_blocks - unfreeze_last_n_blocks:
                            should_freeze_backbone = False
                
                if should_freeze_backbone:
                    param.requires_grad = False
                    stats['frozen'] += param.numel()
                else:
                    param.requires_grad = True
                    stats['trainable'] += param.numel()
                stats['total_params'] += param.numel()
        else:
            raise ValueError(f"Unknown transfer_mode: {transfer_mode}")
        
        if verbose:
            print(f"  Frozen parameters: {stats['frozen']:,}")
            print(f"  Trainable parameters: {stats['trainable']:,}")
            print(f"  Total parameters: {stats['total_params']:,}")
            print(f"  Trainable ratio: {100*stats['trainable']/stats['total_params']:.2f}%")
        
        return stats
    
    @staticmethod
    def get_optimizer_param_groups(
        model: nn.Module,
        base_lr: float = 1e-4,
        transfer_mode: str = 'adapter_tune'
    ) -> list:
        """
        Get parameter groups with differentiated learning rates
        
        Returns:
            List of dicts for optimizer
        """
        param_groups = []
        
        # Separate parameters by type
        backbone_params = []
        adapter_params = []
        hgcp_params = []
        fda_params = []
        classifier_params = []
        other_params = []
        
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue
            
            name_lower = name.lower()
            
            if any(p in name_lower for p in ['classifier', 'fc']):
                classifier_params.append(param)
            elif any(p in name_lower for p in HGCPFDATransfer.HGCP_PATTERNS):
                hgcp_params.append(param)
            elif any(p in name_lower for p in HGCPFDATransfer.FDA_PATTERNS):
                fda_params.append(param)
            elif any(p in name_lower for p in HGCPFDATransfer.ADAPTER_PATTERNS):
                adapter_params.append(param)
            elif any(p in name_lower for p in HGCPFDATransfer.BACKBONE_PATTERNS):
                backbone_params.append(param)
            else:
                other_params.append(param)
        
        # Create param groups with different learning rates
        lr_scales = {
            'linear_probe': {'backbone': 0.0, 'adapter': 0.0, 'hgcp': 0.0, 'fda': 0.0, 'classifier': 1.0, 'other': 0.0},
            'adapter_tune': {'backbone': 0.1, 'adapter': 1.0, 'hgcp': 1.0, 'fda': 1.0, 'classifier': 2.0, 'other': 1.0},
            'finetune': {'backbone': 0.1, 'adapter': 1.0, 'hgcp': 1.0, 'fda': 1.0, 'classifier': 2.0, 'other': 1.0}
        }
        
        scales = lr_scales.get(transfer_mode, lr_scales['adapter_tune'])
        
        if backbone_params and scales['backbone'] > 0:
            param_groups.append({'params': backbone_params, 'lr': base_lr * scales['backbone'], 'name': 'backbone'})
        if adapter_params and scales['adapter'] > 0:
            param_groups.append({'params': adapter_params, 'lr': base_lr * scales['adapter'], 'name': 'adapter'})
        if hgcp_params and scales['hgcp'] > 0:
            param_groups.append({'params': hgcp_params, 'lr': base_lr * scales['hgcp'], 'name': 'hgcp'})
        if fda_params and scales['fda'] > 0:
            param_groups.append({'params': fda_params, 'lr': base_lr * scales['fda'], 'name': 'fda'})
        if classifier_params and scales['classifier'] > 0:
            param_groups.append({'params': classifier_params, 'lr': base_lr * scales['classifier'], 'name': 'classifier'})
        if other_params and scales['other'] > 0:
            param_groups.append({'params': other_params, 'lr': base_lr * scales['other'], 'name': 'other'})
        
        return param_groups
