"""
Adaptive Environmental Fusion (AEF) - V2
æ”¯æŒå¯é…ç½®çš„ç¯å¢ƒç¼–ç å™¨å±‚æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class AdaptiveEnvironmentalFusion(nn.Module):
    """è‡ªé€‚åº”ç¯å¢ƒç‰¹å¾èåˆæ¨¡å— - ä¿®å¤ç‰ˆ (Fix Double Sigmoid)"""
    
    def __init__(self, img_dim=768, env_dim=27, hidden_dim=2048, num_heads=8, 
                 dropout=0.2, num_layers=3):
        super().__init__()
        
        self.img_dim = img_dim
        self.env_dim = env_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        
        # Input normalization
        self.env_input_norm = nn.LayerNorm(env_dim)
        self.img_input_norm = nn.LayerNorm(img_dim)
        
        # ç¯å¢ƒç¼–ç å™¨
        self.env_encoder = self._build_env_encoder(env_dim, hidden_dim, img_dim, num_layers, dropout)
        
        # Cross-Attention
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=img_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # ğŸ”´ æ ¸å¿ƒä¿®æ”¹ï¼šé—¨æ§ç½‘ç»œç§»é™¤æœ€åçš„ Sigmoid
        # ç°åœ¨çš„è¾“å‡ºæ˜¯ Logits (-inf åˆ° +inf)ï¼Œè€Œä¸æ˜¯ (0, 1)
        self.gate = nn.Sequential(
            nn.Linear(img_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, img_dim),
            nn.LayerNorm(img_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(img_dim, 1)
            # [Deleted] nn.Sigmoid()  <-- ç§»é™¤äº†è¿™è¡Œ
        )
        
        # Temperature (å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°)
        self.temperature = nn.Parameter(torch.tensor(0.1))
        
        self.norm1 = nn.LayerNorm(img_dim)
        self.norm2 = nn.LayerNorm(img_dim)
        
        self._init_weights()
        
        print(f"  AdaptiveEnvironmentalFusion (Fixed): env_dim={env_dim}, hidden_dim={hidden_dim}, "
              f"num_layers={num_layers}, num_heads={num_heads}")
    
    def _build_env_encoder(self, env_dim, hidden_dim, output_dim, num_layers, dropout):
        """æ„å»ºå¯é…ç½®å±‚æ•°çš„ç¯å¢ƒç¼–ç å™¨ (ä¿æŒä¸å˜)"""
        layers = []
        if num_layers == 1:
            layers.extend([
                nn.Linear(env_dim, output_dim),
                nn.LayerNorm(output_dim),
                nn.GELU(),
            ])
        elif num_layers == 2:
            layers.extend([
                nn.Linear(env_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, output_dim),
                nn.LayerNorm(output_dim),
            ])
        else:
            layers.extend([
                nn.Linear(env_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
            ])
            for _ in range(num_layers - 2):
                layers.extend([
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.LayerNorm(hidden_dim),
                    nn.GELU(),
                    nn.Dropout(dropout),
                ])
            layers.extend([
                nn.Linear(hidden_dim, output_dim),
                nn.LayerNorm(output_dim),
            ])
        return nn.Sequential(*layers)
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, img_features, env_features, return_alpha=False):
        """å‰å‘ä¼ æ’­"""
        # Normalize inputs
        env_features = self.env_input_norm(env_features)
        img_features = self.img_input_norm(img_features)
        
        # ç¼–ç ç¯å¢ƒç‰¹å¾
        env_encoded = self.env_encoder(env_features)
        
        # Cross-attention
        img_feat_expanded = img_features.unsqueeze(1)
        env_feat_expanded = env_encoded.unsqueeze(1)
        
        attn_out, _ = self.cross_attn(
            query=img_feat_expanded,
            key=env_feat_expanded,
            value=env_feat_expanded
        )
        
        attn_out = attn_out.squeeze(1)
        attn_out = self.norm1(attn_out + img_features)
        
        # é—¨æ§æœºåˆ¶
        concat_feat = torch.cat([img_features, env_encoded], dim=1)
        gate_logits = self.gate(concat_feat)  # ç°åœ¨æ˜¯ Logits
        
        # æ¸©åº¦ç¼©æ”¾ + sigmoid
        # é€»è¾‘ä¿®å¤ï¼šç°åœ¨ gate_logits å¯ä»¥æ˜¯è´Ÿæ•°
        # å¦‚æœ gate_logits = -5, temp = 0.1 -> -50 -> sigmoid â‰ˆ 0.0 (å®Œå…¨å…³é—­)
        # å¦‚æœ gate_logits = +5, temp = 0.1 -> +50 -> sigmoid â‰ˆ 1.0 (å®Œå…¨å¼€å¯)
        alpha = gate_logits / torch.clamp(self.temperature.abs(), min=0.01)
        alpha = torch.sigmoid(alpha)
        
        # è‡ªé€‚åº”èåˆ
        fused = alpha * attn_out + (1 - alpha) * img_features
        fused = self.norm2(fused)
        
        if return_alpha:
            return fused, gate_logits # è¿”å› logits ä»¥ä¾¿è§‚å¯ŸåŸå§‹å€¼
        return fused
    
    def get_gate_value(self, img_features, env_features):
        """è·å–é—¨æ§ç½‘ç»œçš„åŸå§‹è¾“å‡º Logits"""
        env_features = self.env_input_norm(env_features)
        img_features = self.img_input_norm(img_features)
        env_encoded = self.env_encoder(env_features)
        concat_feat = torch.cat([img_features, env_encoded], dim=1)
        return self.gate(concat_feat)  # è¿”å› Logits


class SimpleConcatFusion(nn.Module):
    """ç®€å•æ‹¼æ¥èåˆ - ç”¨äºå¯¹æ¯”å®éªŒ"""
    
    def __init__(self, img_dim=768, env_dim=27, hidden_dim=256, num_layers=2, dropout=0.1):
        super().__init__()
        
        # ç¯å¢ƒç¼–ç å™¨
        layers = [nn.Linear(env_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.GELU()]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.GELU(), nn.Dropout(dropout)])
        layers.append(nn.Linear(hidden_dim, img_dim))
        
        self.env_encoder = nn.Sequential(*layers)
        self.norm = nn.LayerNorm(img_dim * 2)
        
    def forward(self, img_features, env_features, return_alpha=False):
        env_encoded = self.env_encoder(env_features)
        fused = torch.cat([img_features, env_encoded], dim=-1)
        fused = self.norm(fused)
        if return_alpha:
            return fused, None
        return fused


class NoEnvFusion(nn.Module):
    """ä¸ä½¿ç”¨ç¯å¢ƒç‰¹å¾ - ç”¨äºæ¶ˆèå®éªŒ"""
    
    def __init__(self, img_dim=768, **kwargs):
        super().__init__()
        self.norm = nn.LayerNorm(img_dim)
        
    def forward(self, img_features, env_features=None, return_alpha=False):
        out = self.norm(img_features)
        if return_alpha:
            return out, torch.zeros(img_features.size(0), 1, device=img_features.device)
        return out
